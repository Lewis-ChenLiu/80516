{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca61a1f3-a22f-480c-ae46-8ffcf7a8564a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Causal Discovery in Serum miRNomes: Revealing Biomarker Networks for Early Cancer Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fca209-f115-4105-a5b9-7d3e85b48116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dat = pd.read_csv('data.txt', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b0b0c-381e-4a2d-97ce-7eaebf5391a2",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "BC: Breast cancer\n",
    "\n",
    "BL: Bladder cancer\n",
    "\n",
    "BT: Biliary tract cancer\n",
    "\n",
    "CC: Colorectal cancer\n",
    "\n",
    "EC: Esophageal squamous cell carcinoma\n",
    "\n",
    "GC: Gastric cancer\n",
    "\n",
    "HC: Hepatocellular cancer\n",
    "\n",
    "LK: Lung cancer\n",
    "\n",
    "OV: Ovarian cancer\n",
    "\n",
    "PC: Pancreatic cancer\n",
    "\n",
    "PR: Prostate cancer\n",
    "\n",
    "SA: Sarcoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a24dc5-dd94-44a8-8f9b-c01a5a925aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BC: 706\n",
      "BL: 399\n",
      "BT: 402\n",
      "CC: 1596\n",
      "Control: 5908\n",
      "EC: 566\n",
      "GC: 1418\n",
      "HC: 348\n",
      "LK: 1699\n",
      "OV: 428\n",
      "PC: 851\n",
      "PR: 1257\n",
      "SA: 612\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "y = [\"\".join(re.findall(r'[A-Za-z]+', each)) for each in dat.columns[1:]]\n",
    "\n",
    "cancerlist = ['BC','BL','BT','CC','EC','GC','HC','LK','OV','PC','PR','SA']\n",
    "y = np.array([each if each in cancerlist else 'Control' for each in y])\n",
    "\n",
    "### Count occurrences\n",
    "name_counts = Counter(y)\n",
    "for name, count in name_counts.items():\n",
    "    print(f\"{name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2b45d9-4665-4fbc-a74e-14e9153a7c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hsa-miR-28-3p</th>\n",
       "      <th>hsa-miR-27a-5p</th>\n",
       "      <th>hsa-miR-518b</th>\n",
       "      <th>hsa-miR-520b</th>\n",
       "      <th>hsa-miR-498</th>\n",
       "      <th>hsa-miR-512-3p</th>\n",
       "      <th>hsa-miR-491-5p</th>\n",
       "      <th>hsa-miR-490-3p</th>\n",
       "      <th>hsa-miR-452-5p</th>\n",
       "      <th>hsa-miR-451a</th>\n",
       "      <th>...</th>\n",
       "      <th>hsa-miR-3606-3p</th>\n",
       "      <th>hsa-miR-1292-3p</th>\n",
       "      <th>hsa-miR-6889-5p</th>\n",
       "      <th>hsa-miR-6888-3p</th>\n",
       "      <th>hsa-miR-6881-5p</th>\n",
       "      <th>hsa-miR-6880-3p</th>\n",
       "      <th>hsa-miR-6873-5p</th>\n",
       "      <th>hsa-miR-6872-3p</th>\n",
       "      <th>hsa-miR-6865-5p</th>\n",
       "      <th>hsa-miR-6864-3p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>2.839246</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>6.80529</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "      <td>-1.177703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.556806</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>4.716967</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>3.944541</td>\n",
       "      <td>6.959148</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>5.41015</td>\n",
       "      <td>-1.556806</td>\n",
       "      <td>5.877905</td>\n",
       "      <td>1.045616</td>\n",
       "      <td>-1.556806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.321168</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>4.327931</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>6.477432</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>4.881499</td>\n",
       "      <td>6.729782</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>6.252127</td>\n",
       "      <td>-1.321168</td>\n",
       "      <td>4.664817</td>\n",
       "      <td>3.973792</td>\n",
       "      <td>-1.321168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.176251</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>4.970866</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>6.48354</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>3.02212</td>\n",
       "      <td>7.05241</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>5.719362</td>\n",
       "      <td>-2.176251</td>\n",
       "      <td>4.472891</td>\n",
       "      <td>4.943061</td>\n",
       "      <td>-2.176251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>1.726077</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>2.708338</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-0.747342</td>\n",
       "      <td>6.497237</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>5.140446</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "      <td>-2.079343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16185</th>\n",
       "      <td>3.867679</td>\n",
       "      <td>3.501322</td>\n",
       "      <td>2.837191</td>\n",
       "      <td>0.665455</td>\n",
       "      <td>4.803604</td>\n",
       "      <td>1.130736</td>\n",
       "      <td>6.193152</td>\n",
       "      <td>0.41655</td>\n",
       "      <td>-0.498561</td>\n",
       "      <td>1.665464</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.540565</td>\n",
       "      <td>4.7634</td>\n",
       "      <td>7.220702</td>\n",
       "      <td>-5.540565</td>\n",
       "      <td>3.089055</td>\n",
       "      <td>4.941072</td>\n",
       "      <td>-0.742389</td>\n",
       "      <td>4.942069</td>\n",
       "      <td>4.134535</td>\n",
       "      <td>-5.540565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16186</th>\n",
       "      <td>5.311947</td>\n",
       "      <td>4.073188</td>\n",
       "      <td>4.101201</td>\n",
       "      <td>2.118686</td>\n",
       "      <td>5.698477</td>\n",
       "      <td>2.344266</td>\n",
       "      <td>4.712554</td>\n",
       "      <td>0.785063</td>\n",
       "      <td>-4.417322</td>\n",
       "      <td>2.849492</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.417322</td>\n",
       "      <td>5.446265</td>\n",
       "      <td>7.163131</td>\n",
       "      <td>-4.417322</td>\n",
       "      <td>2.154039</td>\n",
       "      <td>4.884964</td>\n",
       "      <td>-4.417322</td>\n",
       "      <td>4.718529</td>\n",
       "      <td>3.350626</td>\n",
       "      <td>-4.417322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16187</th>\n",
       "      <td>2.229432</td>\n",
       "      <td>2.674183</td>\n",
       "      <td>3.867016</td>\n",
       "      <td>3.424473</td>\n",
       "      <td>5.689167</td>\n",
       "      <td>3.830466</td>\n",
       "      <td>5.090698</td>\n",
       "      <td>1.019828</td>\n",
       "      <td>0.885724</td>\n",
       "      <td>6.267623</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.324501</td>\n",
       "      <td>5.002374</td>\n",
       "      <td>7.283329</td>\n",
       "      <td>-4.324501</td>\n",
       "      <td>2.489638</td>\n",
       "      <td>5.280215</td>\n",
       "      <td>0.143643</td>\n",
       "      <td>4.764032</td>\n",
       "      <td>3.781309</td>\n",
       "      <td>-1.050319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16188</th>\n",
       "      <td>2.249489</td>\n",
       "      <td>-0.639963</td>\n",
       "      <td>0.423182</td>\n",
       "      <td>1.028599</td>\n",
       "      <td>5.844544</td>\n",
       "      <td>0.472641</td>\n",
       "      <td>5.284079</td>\n",
       "      <td>-0.795367</td>\n",
       "      <td>-4.01753</td>\n",
       "      <td>5.966505</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.01753</td>\n",
       "      <td>4.99631</td>\n",
       "      <td>7.052476</td>\n",
       "      <td>-4.01753</td>\n",
       "      <td>1.27836</td>\n",
       "      <td>4.922518</td>\n",
       "      <td>-0.396838</td>\n",
       "      <td>4.909489</td>\n",
       "      <td>3.475894</td>\n",
       "      <td>-4.01753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16189</th>\n",
       "      <td>3.507044</td>\n",
       "      <td>1.620584</td>\n",
       "      <td>4.379823</td>\n",
       "      <td>4.495673</td>\n",
       "      <td>5.990411</td>\n",
       "      <td>0.487298</td>\n",
       "      <td>5.202536</td>\n",
       "      <td>3.264302</td>\n",
       "      <td>3.086127</td>\n",
       "      <td>10.347489</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.681226</td>\n",
       "      <td>5.495473</td>\n",
       "      <td>6.853078</td>\n",
       "      <td>-2.681226</td>\n",
       "      <td>1.477434</td>\n",
       "      <td>4.855908</td>\n",
       "      <td>-2.681226</td>\n",
       "      <td>5.144052</td>\n",
       "      <td>4.529653</td>\n",
       "      <td>1.454712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16190 rows × 2565 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hsa-miR-28-3p hsa-miR-27a-5p hsa-miR-518b hsa-miR-520b hsa-miR-498  \\\n",
       "0         -1.177703      -1.177703    -1.177703    -1.177703    2.839246   \n",
       "1         -1.556806      -1.556806    -1.556806    -1.556806    4.716967   \n",
       "2         -1.321168      -1.321168    -1.321168    -1.321168    4.327931   \n",
       "3         -2.176251      -2.176251    -2.176251    -2.176251    4.970866   \n",
       "4         -2.079343      -2.079343    -2.079343    -2.079343    1.726077   \n",
       "...             ...            ...          ...          ...         ...   \n",
       "16185      3.867679       3.501322     2.837191     0.665455    4.803604   \n",
       "16186      5.311947       4.073188     4.101201     2.118686    5.698477   \n",
       "16187      2.229432       2.674183     3.867016     3.424473    5.689167   \n",
       "16188      2.249489      -0.639963     0.423182     1.028599    5.844544   \n",
       "16189      3.507044       1.620584     4.379823     4.495673    5.990411   \n",
       "\n",
       "      hsa-miR-512-3p hsa-miR-491-5p hsa-miR-490-3p hsa-miR-452-5p  \\\n",
       "0          -1.177703      -1.177703      -1.177703      -1.177703   \n",
       "1          -1.556806      -1.556806      -1.556806      -1.556806   \n",
       "2          -1.321168      -1.321168      -1.321168      -1.321168   \n",
       "3          -2.176251      -2.176251      -2.176251      -2.176251   \n",
       "4          -2.079343      -2.079343      -2.079343      -2.079343   \n",
       "...              ...            ...            ...            ...   \n",
       "16185       1.130736       6.193152        0.41655      -0.498561   \n",
       "16186       2.344266       4.712554       0.785063      -4.417322   \n",
       "16187       3.830466       5.090698       1.019828       0.885724   \n",
       "16188       0.472641       5.284079      -0.795367       -4.01753   \n",
       "16189       0.487298       5.202536       3.264302       3.086127   \n",
       "\n",
       "      hsa-miR-451a  ... hsa-miR-3606-3p hsa-miR-1292-3p hsa-miR-6889-5p  \\\n",
       "0        -1.177703  ...       -1.177703       -1.177703         6.80529   \n",
       "1        -1.556806  ...       -1.556806        3.944541        6.959148   \n",
       "2         6.477432  ...       -1.321168        4.881499        6.729782   \n",
       "3          6.48354  ...       -2.176251         3.02212         7.05241   \n",
       "4         2.708338  ...       -2.079343       -0.747342        6.497237   \n",
       "...            ...  ...             ...             ...             ...   \n",
       "16185     1.665464  ...       -5.540565          4.7634        7.220702   \n",
       "16186     2.849492  ...       -4.417322        5.446265        7.163131   \n",
       "16187     6.267623  ...       -4.324501        5.002374        7.283329   \n",
       "16188     5.966505  ...        -4.01753         4.99631        7.052476   \n",
       "16189    10.347489  ...       -2.681226        5.495473        6.853078   \n",
       "\n",
       "      hsa-miR-6888-3p hsa-miR-6881-5p hsa-miR-6880-3p hsa-miR-6873-5p  \\\n",
       "0           -1.177703       -1.177703        0.016052       -1.177703   \n",
       "1           -1.556806       -1.556806         5.41015       -1.556806   \n",
       "2           -1.321168       -1.321168        6.252127       -1.321168   \n",
       "3           -2.176251       -2.176251        5.719362       -2.176251   \n",
       "4           -2.079343       -2.079343        5.140446       -2.079343   \n",
       "...               ...             ...             ...             ...   \n",
       "16185       -5.540565        3.089055        4.941072       -0.742389   \n",
       "16186       -4.417322        2.154039        4.884964       -4.417322   \n",
       "16187       -4.324501        2.489638        5.280215        0.143643   \n",
       "16188        -4.01753         1.27836        4.922518       -0.396838   \n",
       "16189       -2.681226        1.477434        4.855908       -2.681226   \n",
       "\n",
       "      hsa-miR-6872-3p hsa-miR-6865-5p hsa-miR-6864-3p  \n",
       "0           -1.177703       -1.177703       -1.177703  \n",
       "1            5.877905        1.045616       -1.556806  \n",
       "2            4.664817        3.973792       -1.321168  \n",
       "3            4.472891        4.943061       -2.176251  \n",
       "4           -2.079343       -2.079343       -2.079343  \n",
       "...               ...             ...             ...  \n",
       "16185        4.942069        4.134535       -5.540565  \n",
       "16186        4.718529        3.350626       -4.417322  \n",
       "16187        4.764032        3.781309       -1.050319  \n",
       "16188        4.909489        3.475894        -4.01753  \n",
       "16189        5.144052        4.529653        1.454712  \n",
       "\n",
       "[16190 rows x 2565 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dat.transpose()\n",
    "X = X.rename(columns = X.iloc[0]) \n",
    "X = X.drop(X.index[0])             \n",
    "X = X.reset_index(drop = True)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdbda7f-bc90-45c1-8ab3-349d2207d547",
   "metadata": {},
   "source": [
    "#### Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f25a203c-2b99-41ae-a07d-7e4946bffbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC (7504, 50) 7504\n",
      "GC (7326, 50) 7326\n",
      "LK (7607, 50) 7607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def dataset(name = 'CC', n_features = 50):\n",
    "    \n",
    "    ### Select data\n",
    "    index = np.isin(y, [name, 'Control'])\n",
    "    _X, _y = X.iloc[index,], y[index]\n",
    "    _y = 1 * (_y != 'Control')\n",
    "    \n",
    "    ### Fit RF to get the top 100 important features\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators = 200,\n",
    "        random_state = 42,\n",
    "    )\n",
    "    clf.fit(_X, _y)\n",
    "\n",
    "    importances = clf.feature_importances_\n",
    "    idx_sorted = np.argsort(importances)[::-1]\n",
    "    top_idx = idx_sorted[:n_features]\n",
    "    top_features = _X.columns[top_idx]\n",
    "    _X_reduced = _X[top_features].copy()\n",
    "    \n",
    "    return _X_reduced, _y\n",
    "\n",
    "CC_X, CC_y = dataset('CC')\n",
    "GC_X, GC_y = dataset('GC')\n",
    "LU_X, LU_y = dataset('LK')\n",
    "\n",
    "print('CC', CC_X.shape, len(CC_y))\n",
    "print('GC', GC_X.shape, len(GC_y))\n",
    "print('LK', LU_X.shape, len(LU_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b33a03-9e15-4176-abed-ee5f78ddc6d9",
   "metadata": {},
   "source": [
    "### CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ccfbba-316c-4ffb-af6b-82390e812345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "              Model  Precision   Recall  F1 Score\n",
      "Logistic Regression   0.986301 0.982456  0.984375\n",
      "         Linear SVM   0.988281 0.986355  0.987317\n",
      "      Random Forest   0.971374 0.992203  0.981678\n",
      "  Gradient Boosting   0.971264 0.988304  0.979710\n",
      "\n",
      "Top 10 Features — Logistic Regression:\n",
      "hsa-miR-1228-5p    2.176648\n",
      "hsa-miR-3180       1.654565\n",
      "hsa-miR-4730       1.574396\n",
      "hsa-miR-614        1.369362\n",
      "hsa-miR-6787-5p    1.359926\n",
      "hsa-miR-6717-5p    1.356550\n",
      "hsa-miR-6765-5p    1.334836\n",
      "hsa-miR-211-3p     1.212059\n",
      "hsa-miR-6802-5p    1.211165\n",
      "hsa-miR-885-3p     1.130723\n",
      "\n",
      "Top 10 Features — Linear SVM:\n",
      "hsa-miR-1228-5p    1.645746\n",
      "hsa-miR-4730       1.130794\n",
      "hsa-miR-6802-5p    1.078796\n",
      "hsa-miR-3180       1.028580\n",
      "hsa-miR-6787-5p    0.974077\n",
      "hsa-miR-6717-5p    0.929998\n",
      "hsa-miR-614        0.796794\n",
      "hsa-miR-6756-5p    0.782767\n",
      "hsa-miR-1469       0.726079\n",
      "hsa-miR-4783-3p    0.724582\n",
      "\n",
      "Top 10 Features — Random Forest:\n",
      "hsa-miR-4783-3p    0.150539\n",
      "hsa-miR-1228-5p    0.146640\n",
      "hsa-miR-4730       0.092459\n",
      "hsa-miR-3940-5p    0.076330\n",
      "hsa-miR-1307-3p    0.057850\n",
      "hsa-miR-663a       0.051673\n",
      "hsa-miR-320b       0.050819\n",
      "hsa-miR-6784-5p    0.043504\n",
      "hsa-miR-3184-5p    0.037758\n",
      "hsa-miR-320a       0.034795\n",
      "\n",
      "Top 10 Features — Gradient Boosting:\n",
      "hsa-miR-3940-5p    0.441259\n",
      "hsa-miR-4730       0.194079\n",
      "hsa-miR-4783-3p    0.116754\n",
      "hsa-miR-1228-5p    0.102325\n",
      "hsa-miR-885-3p     0.028038\n",
      "hsa-miR-6717-5p    0.020200\n",
      "hsa-miR-320b       0.016219\n",
      "hsa-miR-1233-5p    0.014727\n",
      "hsa-miR-4533       0.009095\n",
      "hsa-miR-1307-3p    0.008924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(CC_X, CC_y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "### Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(max_iter = 1000, random_state = 42)\n",
    "    ),\n",
    "    'Linear SVM': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(kernel = 'linear', probability = True, random_state = 42)\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators = 100, random_state = 42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state = 42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "feature_importances = {}\n",
    "\n",
    "### Train, predict, and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec  = recall_score(y_test, y_pred)\n",
    "    f1   = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({'Model': name, 'Precision': prec, 'Recall': rec, 'F1 Score': f1})\n",
    "    \n",
    "    if isinstance(model, Pipeline):\n",
    "        model = model.steps[-1][1]\n",
    "        \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        importances = abs(model.coef_)[0]\n",
    "    \n",
    "    feat_imp_series = pd.Series(importances, index = CC_X.columns)\n",
    "    top10 = feat_imp_series.sort_values(ascending = False).head(10)\n",
    "    feature_importances[name] = top10\n",
    "\n",
    "### Display the results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print('\\nModel Performance:')\n",
    "print(results_df.to_string(index = False))\n",
    "\n",
    "### Display top 10 features for each model\n",
    "for model_name, top_feats in feature_importances.items():\n",
    "    print(f'\\nTop 10 Features — {model_name}:')\n",
    "    print(top_feats.to_string(header=['Importance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7020cb-545c-41bc-a30a-38bc35a6edef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8c214ea75c47cb8e3b98c09444e417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Nodes:\n",
      "X1;X2;X3;X4;X5;X6;X7;X8;X9;X10;X11;X12;X13;X14;X15;X16;X17;X18;X19;X20;X21;X22;X23;X24;X25;X26;X27;X28;X29;X30;X31;X32;X33;X34;X35;X36;X37;X38;X39;X40;X41;X42;X43;X44;X45;X46;X47;X48;X49;X50;X51\n",
      "\n",
      "Graph Edges:\n",
      "1. X2 --> X1\n",
      "2. X1 --> X4\n",
      "3. X7 --> X1\n",
      "4. X1 --> X15\n",
      "5. X35 --> X1\n",
      "6. X51 --> X1\n",
      "7. X2 --> X16\n",
      "8. X19 --> X2\n",
      "9. X20 --> X2\n",
      "10. X22 --> X2\n",
      "11. X2 --> X26\n",
      "12. X31 --> X2\n",
      "13. X34 --> X2\n",
      "14. X51 --> X2\n",
      "15. X3 --> X9\n",
      "16. X16 --> X3\n",
      "17. X3 --> X19\n",
      "18. X3 --- X26\n",
      "19. X27 --> X3\n",
      "20. X32 --> X3\n",
      "21. X3 --- X38\n",
      "22. X8 --> X4\n",
      "23. X10 --> X4\n",
      "24. X22 --> X4\n",
      "25. X23 --> X4\n",
      "26. X35 --> X4\n",
      "27. X51 --> X4\n",
      "28. X8 --> X5\n",
      "29. X5 --> X14\n",
      "30. X5 --> X25\n",
      "31. X5 --> X29\n",
      "32. X35 --> X5\n",
      "33. X6 --> X7\n",
      "34. X11 --> X6\n",
      "35. X12 --> X6\n",
      "36. X6 --> X37\n",
      "37. X39 --> X6\n",
      "38. X40 --> X6\n",
      "39. X44 --> X6\n",
      "40. X13 --> X7\n",
      "41. X39 --> X7\n",
      "42. X42 --> X7\n",
      "43. X45 --> X7\n",
      "44. X8 --> X13\n",
      "45. X25 --> X8\n",
      "46. X28 --> X8\n",
      "47. X29 --> X8\n",
      "48. X43 --> X8\n",
      "49. X26 --> X9\n",
      "50. X29 --> X9\n",
      "51. X32 --> X9\n",
      "52. X9 --- X38\n",
      "53. X49 --> X9\n",
      "54. X14 --> X10\n",
      "55. X20 --> X10\n",
      "56. X35 --> X10\n",
      "57. X42 --> X10\n",
      "58. X44 --> X10\n",
      "59. X14 --> X11\n",
      "60. X17 --> X11\n",
      "61. X11 --> X45\n",
      "62. X16 --> X12\n",
      "63. X24 --> X12\n",
      "64. X26 --> X12\n",
      "65. X33 --> X12\n",
      "66. X43 --> X12\n",
      "67. X50 --> X12\n",
      "68. X51 --> X12\n",
      "69. X28 --> X13\n",
      "70. X29 --> X13\n",
      "71. X20 --> X14\n",
      "72. X30 --> X14\n",
      "73. X35 --> X14\n",
      "74. X19 --> X15\n",
      "75. X15 --> X26\n",
      "76. X27 --> X15\n",
      "77. X43 --> X15\n",
      "78. X47 --> X15\n",
      "79. X48 --> X15\n",
      "80. X18 --> X16\n",
      "81. X19 --> X16\n",
      "82. X21 --> X16\n",
      "83. X24 --> X16\n",
      "84. X16 --- X38\n",
      "85. X41 --> X16\n",
      "86. X21 --> X17\n",
      "87. X22 --> X17\n",
      "88. X17 --> X23\n",
      "89. X24 --> X17\n",
      "90. X17 --> X31\n",
      "91. X17 --> X42\n",
      "92. X44 --> X17\n",
      "93. X46 --> X17\n",
      "94. X18 --> X26\n",
      "95. X27 --> X18\n",
      "96. X28 --> X18\n",
      "97. X47 --> X18\n",
      "98. X18 --> X48\n",
      "99. X18 --> X50\n",
      "100. X51 --> X18\n",
      "101. X36 --> X19\n",
      "102. X41 --> X19\n",
      "103. X19 --> X48\n",
      "104. X21 --> X23\n",
      "105. X21 --- X41\n",
      "106. X21 --> X46\n",
      "107. X22 --> X23\n",
      "108. X22 --> X42\n",
      "109. X22 --> X46\n",
      "110. X39 --> X23\n",
      "111. X44 --> X23\n",
      "112. X31 --> X24\n",
      "113. X24 --> X33\n",
      "114. X38 --> X24\n",
      "115. X41 --> X24\n",
      "116. X29 --> X25\n",
      "117. X32 --> X25\n",
      "118. X44 --> X25\n",
      "119. X49 --> X25\n",
      "120. X50 --> X25\n",
      "121. X26 --> X31\n",
      "122. X26 --> X33\n",
      "123. X26 --> X37\n",
      "124. X26 --> X38\n",
      "125. X34 --> X27\n",
      "126. X27 --> X36\n",
      "127. X51 --> X27\n",
      "128. X28 --> X30\n",
      "129. X28 --- X42\n",
      "130. X28 --> X45\n",
      "131. X28 --> X46\n",
      "132. X29 --> X32\n",
      "133. X34 --> X29\n",
      "134. X35 --> X30\n",
      "135. X46 --> X30\n",
      "136. X49 --> X30\n",
      "137. X31 --> X33\n",
      "138. X31 --> X37\n",
      "139. X41 --> X31\n",
      "140. X46 --> X31\n",
      "141. X32 --> X49\n",
      "142. X32 --> X50\n",
      "143. X33 --> X37\n",
      "144. X41 --> X33\n",
      "145. X43 --> X33\n",
      "146. X34 --> X36\n",
      "147. X34 --- X48\n",
      "148. X34 --- X49\n",
      "149. X36 --> X40\n",
      "150. X36 --> X45\n",
      "151. X48 --> X36\n",
      "152. X41 --> X37\n",
      "153. X43 --> X40\n",
      "154. X40 --> X45\n",
      "155. X48 --> X40\n",
      "156. X40 --> X49\n",
      "157. X50 --> X40\n",
      "158. X41 --> X43\n",
      "159. X47 --> X43\n",
      "160. X44 --> X46\n",
      "161. X46 --> X45\n",
      "162. X45 --> X47\n",
      "163. X47 --- X50\n",
      "164. X50 --> X48\n",
      "165. X50 --> X49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "\n",
    "### Dataset\n",
    "df = pd.concat([CC_X, pd.Series(CC_y, name = 'Y', index = CC_X.index)], axis = 1)\n",
    "df = df.astype(float)\n",
    "data = df.values\n",
    "var_names = df.columns.tolist()\n",
    "\n",
    "### PC algorithm\n",
    "cg = pc(data, alpha = 0.05, labels = var_names, max_k = 2)\n",
    "print(cg.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f75523-f479-4d66-b8f9-58680404dfba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(var_names).to_csv('CC_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455b804f-713e-4913-9762-61633f5ad12a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from causallearn.search.FCMBased import lingam\n",
    "\n",
    "model = lingam.DirectLiNGAM()\n",
    "model.fit(data)\n",
    "pd.DataFrame(model.adjacency_matrix_).to_csv('CC_lingam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047c6266-6e2e-46ad-9aaa-e869df6e7b88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from notears.linear import notears_linear\n",
    "\n",
    "W_est = notears_linear(data, lambda1 = 0.0001, loss_type = 'l2')\n",
    "\n",
    "pd.DataFrame(W_est).to_csv('CC_notears.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648b1cb-aa6c-48d9-a009-a209ceb0a2d7",
   "metadata": {},
   "source": [
    "### GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e632a93-2b98-4438-a454-1087c1f0db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "              Model  Precision   Recall  F1 Score\n",
      "Logistic Regression   0.993197 0.993197  0.993197\n",
      "         Linear SVM   0.986517 0.995465  0.990971\n",
      "      Random Forest   0.993213 0.995465  0.994337\n",
      "  Gradient Boosting   0.990991 0.997732  0.994350\n",
      "\n",
      "Top 10 Features — Logistic Regression:\n",
      "hsa-miR-4730       1.865394\n",
      "hsa-miR-3184-5p    1.603938\n",
      "hsa-miR-1228-5p    1.310323\n",
      "hsa-miR-6781-5p    0.911518\n",
      "hsa-miR-663a       0.903049\n",
      "hsa-miR-320a       0.837229\n",
      "hsa-miR-6088       0.827025\n",
      "hsa-miR-575        0.813656\n",
      "hsa-miR-17-3p      0.786541\n",
      "hsa-miR-1307-3p    0.670886\n",
      "\n",
      "Top 10 Features — Linear SVM:\n",
      "hsa-miR-3184-5p    0.922888\n",
      "hsa-miR-4730       0.773812\n",
      "hsa-miR-1228-5p    0.539167\n",
      "hsa-miR-6088       0.495571\n",
      "hsa-miR-1203       0.422070\n",
      "hsa-miR-6802-5p    0.399914\n",
      "hsa-miR-6781-5p    0.388534\n",
      "hsa-miR-663a       0.346766\n",
      "hsa-miR-320a       0.346199\n",
      "hsa-miR-17-3p      0.340894\n",
      "\n",
      "Top 10 Features — Random Forest:\n",
      "hsa-miR-1228-5p    0.160978\n",
      "hsa-miR-3184-5p    0.131608\n",
      "hsa-miR-663a       0.129702\n",
      "hsa-miR-3940-5p    0.085106\n",
      "hsa-miR-6802-5p    0.060091\n",
      "hsa-miR-4730       0.054084\n",
      "hsa-miR-6088       0.052039\n",
      "hsa-miR-6729-5p    0.035975\n",
      "hsa-miR-1307-3p    0.034343\n",
      "hsa-miR-6786-5p    0.031433\n",
      "\n",
      "Top 10 Features — Gradient Boosting:\n",
      "hsa-miR-3184-5p    0.667626\n",
      "hsa-miR-1228-5p    0.269510\n",
      "hsa-miR-6781-5p    0.021892\n",
      "hsa-miR-663a       0.010276\n",
      "hsa-miR-4730       0.008949\n",
      "hsa-miR-6786-5p    0.006841\n",
      "hsa-miR-6729-5p    0.003708\n",
      "hsa-miR-6765-5p    0.002950\n",
      "hsa-miR-1469       0.002264\n",
      "hsa-miR-1277-3p    0.001913\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(GC_X, GC_y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "### Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(max_iter = 1000, random_state = 42)\n",
    "    ),\n",
    "    'Linear SVM': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(kernel = 'linear', probability = True, random_state = 42)\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators = 100, random_state = 42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state = 42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "feature_importances = {}\n",
    "\n",
    "### Train, predict, and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec  = recall_score(y_test, y_pred)\n",
    "    f1   = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({'Model': name, 'Precision': prec, 'Recall': rec, 'F1 Score': f1})\n",
    "    \n",
    "    if isinstance(model, Pipeline):\n",
    "        model = model.steps[-1][1]\n",
    "        \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        importances = abs(model.coef_)[0]\n",
    "    \n",
    "    feat_imp_series = pd.Series(importances, index = CC_X.columns)\n",
    "    top10 = feat_imp_series.sort_values(ascending = False).head(10)\n",
    "    feature_importances[name] = top10\n",
    "\n",
    "### Display the results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print('\\nModel Performance:')\n",
    "print(results_df.to_string(index = False))\n",
    "\n",
    "### Display top 10 features for each model\n",
    "for model_name, top_feats in feature_importances.items():\n",
    "    print(f'\\nTop 10 Features — {model_name}:')\n",
    "    print(top_feats.to_string(header=['Importance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87fdbc6d-5d9f-4be9-b8c1-7804e12c8dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99130a27d02a4c05bc4ca6e0bdb19894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Nodes:\n",
      "X1;X2;X3;X4;X5;X6;X7;X8;X9;X10;X11;X12;X13;X14;X15;X16;X17;X18;X19;X20;X21;X22;X23;X24;X25;X26;X27;X28;X29;X30;X31;X32;X33;X34;X35;X36;X37;X38;X39;X40;X41;X42;X43;X44;X45;X46;X47;X48;X49;X50;X51\n",
      "\n",
      "Graph Edges:\n",
      "1. X11 --> X1\n",
      "2. X13 --> X1\n",
      "3. X14 --> X1\n",
      "4. X1 --> X17\n",
      "5. X30 --> X1\n",
      "6. X31 --> X1\n",
      "7. X1 --> X32\n",
      "8. X42 --> X1\n",
      "9. X44 --> X1\n",
      "10. X1 --> X45\n",
      "11. X5 --> X2\n",
      "12. X2 --> X6\n",
      "13. X17 --> X2\n",
      "14. X38 --> X2\n",
      "15. X51 --> X2\n",
      "16. X13 --> X3\n",
      "17. X14 --> X3\n",
      "18. X20 --> X3\n",
      "19. X21 --> X3\n",
      "20. X3 --> X22\n",
      "21. X3 --> X23\n",
      "22. X25 --> X3\n",
      "23. X49 --> X3\n",
      "24. X51 --> X3\n",
      "25. X5 --> X4\n",
      "26. X18 --> X4\n",
      "27. X33 --> X4\n",
      "28. X44 --> X4\n",
      "29. X47 --> X4\n",
      "30. X51 --> X4\n",
      "31. X7 --> X5\n",
      "32. X36 --> X5\n",
      "33. X42 --> X5\n",
      "34. X46 --> X5\n",
      "35. X51 --> X5\n",
      "36. X21 --> X6\n",
      "37. X36 --> X6\n",
      "38. X37 --> X6\n",
      "39. X51 --> X6\n",
      "40. X15 --> X7\n",
      "41. X17 --> X7\n",
      "42. X26 --> X7\n",
      "43. X32 --> X7\n",
      "44. X33 --> X7\n",
      "45. X7 --> X35\n",
      "46. X11 --> X8\n",
      "47. X8 --> X12\n",
      "48. X13 --> X8\n",
      "49. X18 --> X8\n",
      "50. X8 --> X28\n",
      "51. X38 --> X8\n",
      "52. X42 --> X8\n",
      "53. X8 --> X49\n",
      "54. X13 --> X9\n",
      "55. X15 --> X9\n",
      "56. X9 --> X21\n",
      "57. X9 --> X23\n",
      "58. X9 --> X25\n",
      "59. X30 --> X9\n",
      "60. X38 --> X9\n",
      "61. X41 --> X9\n",
      "62. X10 --> X20\n",
      "63. X10 --> X30\n",
      "64. X10 --> X31\n",
      "65. X10 --> X39\n",
      "66. X44 --> X10\n",
      "67. X10 --> X45\n",
      "68. X50 --> X10\n",
      "69. X13 --> X11\n",
      "70. X18 --> X11\n",
      "71. X25 --> X11\n",
      "72. X28 --> X11\n",
      "73. X41 --> X11\n",
      "74. X44 --> X11\n",
      "75. X47 --> X11\n",
      "76. X12 --> X17\n",
      "77. X12 --> X35\n",
      "78. X41 --> X12\n",
      "79. X12 --> X49\n",
      "80. X18 --> X13\n",
      "81. X13 --> X28\n",
      "82. X34 --> X13\n",
      "83. X13 --> X38\n",
      "84. X48 --> X13\n",
      "85. X20 --> X14\n",
      "86. X21 --> X14\n",
      "87. X14 --> X22\n",
      "88. X29 --> X14\n",
      "89. X14 --> X32\n",
      "90. X45 --> X14\n",
      "91. X47 --> X14\n",
      "92. X15 --> X17\n",
      "93. X15 --> X24\n",
      "94. X38 --> X15\n",
      "95. X16 --> X24\n",
      "96. X16 --> X29\n",
      "97. X16 --> X34\n",
      "98. X16 --> X49\n",
      "99. X17 --> X32\n",
      "100. X35 --> X17\n",
      "101. X51 --> X17\n",
      "102. X18 --> X34\n",
      "103. X18 --> X42\n",
      "104. X18 --- X48\n",
      "105. X19 --> X21\n",
      "106. X19 --> X26\n",
      "107. X31 --> X19\n",
      "108. X19 --> X40\n",
      "109. X19 --> X41\n",
      "110. X47 --> X19\n",
      "111. X50 --> X19\n",
      "112. X20 --> X22\n",
      "113. X20 --> X39\n",
      "114. X20 --> X40\n",
      "115. X41 --> X20\n",
      "116. X44 --> X20\n",
      "117. X20 --> X45\n",
      "118. X51 --> X20\n",
      "119. X25 --> X21\n",
      "120. X30 --> X21\n",
      "121. X32 --> X21\n",
      "122. X43 --> X21\n",
      "123. X45 --> X21\n",
      "124. X47 --> X21\n",
      "125. X50 --> X21\n",
      "126. X25 --> X22\n",
      "127. X22 --> X28\n",
      "128. X38 --> X22\n",
      "129. X39 --> X22\n",
      "130. X42 --> X22\n",
      "131. X26 --> X23\n",
      "132. X23 --> X40\n",
      "133. X41 --> X23\n",
      "134. X43 --> X23\n",
      "135. X26 --> X24\n",
      "136. X27 --> X24\n",
      "137. X24 --> X33\n",
      "138. X34 --> X24\n",
      "139. X35 --> X24\n",
      "140. X44 --> X24\n",
      "141. X47 --> X24\n",
      "142. X24 --> X49\n",
      "143. X30 --> X25\n",
      "144. X39 --> X25\n",
      "145. X44 --> X25\n",
      "146. X45 --> X25\n",
      "147. X26 --> X40\n",
      "148. X26 --> X43\n",
      "149. X33 --> X27\n",
      "150. X35 --> X27\n",
      "151. X37 --> X27\n",
      "152. X46 --> X27\n",
      "153. X29 --> X28\n",
      "154. X38 --> X28\n",
      "155. X41 --> X28\n",
      "156. X42 --> X28\n",
      "157. X43 --> X28\n",
      "158. X34 --> X29\n",
      "159. X49 --> X29\n",
      "160. X30 --> X31\n",
      "161. X30 --> X40\n",
      "162. X43 --> X30\n",
      "163. X44 --> X30\n",
      "164. X31 --> X32\n",
      "165. X47 --> X31\n",
      "166. X50 --> X31\n",
      "167. X47 --> X32\n",
      "168. X38 --> X33\n",
      "169. X48 --> X33\n",
      "170. X48 --> X34\n",
      "171. X34 --> X49\n",
      "172. X35 --- X47\n",
      "173. X49 --> X35\n",
      "174. X36 --- X37\n",
      "175. X37 --- X46\n",
      "176. X38 --> X49\n",
      "177. X43 --> X39\n",
      "178. X39 --> X45\n",
      "179. X50 --> X39\n",
      "180. X41 --> X40\n",
      "181. X45 --> X40\n",
      "182. X41 --> X43\n",
      "183. X44 --> X42\n",
      "184. X50 --> X43\n",
      "185. X43 --> X51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "\n",
    "### Dataset\n",
    "df = pd.concat([GC_X, pd.Series(GC_y, name = 'Y', index = GC_X.index)], axis = 1)\n",
    "df = df.astype(float)\n",
    "data = df.values\n",
    "var_names = df.columns.tolist()\n",
    "\n",
    "### PC algorithm\n",
    "cg = pc(data, alpha = 0.05, labels = var_names, max_k = 2)\n",
    "print(cg.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d8eb7b4-7465-4abc-a7d4-5082e3087814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(var_names).to_csv('GC_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989cdc52-8852-437c-8d55-6ce5f97a8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causallearn.search.FCMBased import lingam\n",
    "\n",
    "model = lingam.DirectLiNGAM()\n",
    "model.fit(data)\n",
    "pd.DataFrame(model.adjacency_matrix_).to_csv('GC_lingam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acd27525-a780-4cba-86ba-dcba0db17e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notears.linear import notears_linear\n",
    "\n",
    "W_est = notears_linear(data, lambda1 = 0.0001, loss_type = 'l2')\n",
    "\n",
    "pd.DataFrame(W_est).to_csv('GC_notears.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166056bd-d9fe-454c-9c27-084d53ffc26d",
   "metadata": {},
   "source": [
    "### LU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dee8e16-5b7a-4d70-8e78-cc472f495343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "              Model  Precision   Recall  F1 Score\n",
      "Logistic Regression   0.994275 0.998084  0.996176\n",
      "         Linear SVM   0.994264 0.996169  0.995215\n",
      "      Random Forest   0.988506 0.988506  0.988506\n",
      "  Gradient Boosting   0.984733 0.988506  0.986616\n",
      "\n",
      "Top 10 Features — Logistic Regression:\n",
      "hsa-miR-6786-5p    2.357817\n",
      "hsa-miR-4730       1.811242\n",
      "hsa-miR-1228-5p    1.479116\n",
      "hsa-miR-1268a      1.274868\n",
      "hsa-miR-6781-5p    1.091330\n",
      "hsa-miR-6729-5p    0.910640\n",
      "hsa-miR-663a       0.846899\n",
      "hsa-miR-3184-5p    0.828001\n",
      "hsa-miR-320a       0.784851\n",
      "hsa-miR-1307-3p    0.754085\n",
      "\n",
      "Top 10 Features — Linear SVM:\n",
      "hsa-miR-6786-5p    1.442459\n",
      "hsa-miR-4730       1.046154\n",
      "hsa-miR-1268a      0.867385\n",
      "hsa-miR-1228-5p    0.849464\n",
      "hsa-miR-3184-5p    0.497034\n",
      "hsa-miR-6781-5p    0.484598\n",
      "hsa-miR-320a       0.462556\n",
      "hsa-miR-614        0.456553\n",
      "hsa-miR-3158-5p    0.448912\n",
      "hsa-miR-6742-5p    0.434073\n",
      "\n",
      "Top 10 Features — Random Forest:\n",
      "hsa-miR-6786-5p    0.163025\n",
      "hsa-miR-1228-5p    0.162683\n",
      "hsa-miR-6729-5p    0.120615\n",
      "hsa-miR-663a       0.071403\n",
      "hsa-miR-6802-5p    0.060920\n",
      "hsa-miR-4730       0.056832\n",
      "hsa-miR-3940-5p    0.056390\n",
      "hsa-miR-6784-5p    0.041036\n",
      "hsa-miR-6742-5p    0.031888\n",
      "hsa-miR-3184-5p    0.030137\n",
      "\n",
      "Top 10 Features — Gradient Boosting:\n",
      "hsa-miR-6786-5p    0.791021\n",
      "hsa-miR-1228-5p    0.101162\n",
      "hsa-miR-3158-5p    0.033519\n",
      "hsa-miR-6729-5p    0.018237\n",
      "hsa-miR-4730       0.014136\n",
      "hsa-miR-4697-5p    0.011118\n",
      "hsa-miR-17-3p      0.005546\n",
      "hsa-miR-6787-5p    0.005449\n",
      "hsa-miR-6802-5p    0.005064\n",
      "hsa-miR-3184-5p    0.003343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(LU_X, LU_y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "### Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(max_iter = 1000, random_state = 42)\n",
    "    ),\n",
    "    'Linear SVM': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(kernel = 'linear', probability = True, random_state = 42)\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators = 100, random_state = 42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state = 42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "feature_importances = {}\n",
    "\n",
    "### Train, predict, and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec  = recall_score(y_test, y_pred)\n",
    "    f1   = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({'Model': name, 'Precision': prec, 'Recall': rec, 'F1 Score': f1})\n",
    "    \n",
    "    if isinstance(model, Pipeline):\n",
    "        model = model.steps[-1][1]\n",
    "        \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        importances = abs(model.coef_)[0]\n",
    "    \n",
    "    feat_imp_series = pd.Series(importances, index = CC_X.columns)\n",
    "    top10 = feat_imp_series.sort_values(ascending = False).head(10)\n",
    "    feature_importances[name] = top10\n",
    "\n",
    "### Display the results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print('\\nModel Performance:')\n",
    "print(results_df.to_string(index = False))\n",
    "\n",
    "### Display top 10 features for each model\n",
    "for model_name, top_feats in feature_importances.items():\n",
    "    print(f'\\nTop 10 Features — {model_name}:')\n",
    "    print(top_feats.to_string(header=['Importance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "798b2430-d810-4d48-85ab-33a5a26a01ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dcafdeba0644fc8270abb4afeff662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Nodes:\n",
      "X1;X2;X3;X4;X5;X6;X7;X8;X9;X10;X11;X12;X13;X14;X15;X16;X17;X18;X19;X20;X21;X22;X23;X24;X25;X26;X27;X28;X29;X30;X31;X32;X33;X34;X35;X36;X37;X38;X39;X40;X41;X42;X43;X44;X45;X46;X47;X48;X49;X50;X51\n",
      "\n",
      "Graph Edges:\n",
      "1. X14 --> X1\n",
      "2. X1 --> X17\n",
      "3. X1 --> X25\n",
      "4. X38 --> X1\n",
      "5. X40 --> X1\n",
      "6. X1 --> X42\n",
      "7. X48 --> X1\n",
      "8. X2 --> X5\n",
      "9. X2 --> X7\n",
      "10. X8 --> X2\n",
      "11. X14 --> X2\n",
      "12. X40 --> X2\n",
      "13. X46 --> X2\n",
      "14. X51 --> X2\n",
      "15. X10 --> X3\n",
      "16. X3 --> X17\n",
      "17. X3 --- X19\n",
      "18. X23 --> X3\n",
      "19. X28 --> X3\n",
      "20. X3 --> X32\n",
      "21. X13 --> X4\n",
      "22. X4 --> X21\n",
      "23. X22 --> X4\n",
      "24. X42 --> X4\n",
      "25. X47 --> X4\n",
      "26. X51 --> X4\n",
      "27. X28 --> X5\n",
      "28. X35 --> X5\n",
      "29. X43 --> X5\n",
      "30. X49 --> X5\n",
      "31. X50 --> X5\n",
      "32. X6 --> X7\n",
      "33. X13 --> X6\n",
      "34. X14 --> X6\n",
      "35. X15 --> X6\n",
      "36. X20 --> X6\n",
      "37. X27 --> X6\n",
      "38. X36 --> X6\n",
      "39. X46 --> X6\n",
      "40. X15 --> X7\n",
      "41. X31 --> X7\n",
      "42. X35 --> X7\n",
      "43. X40 --> X7\n",
      "44. X51 --> X7\n",
      "45. X12 --> X8\n",
      "46. X21 --> X8\n",
      "47. X8 --> X25\n",
      "48. X26 --> X8\n",
      "49. X40 --> X8\n",
      "50. X44 --> X8\n",
      "51. X45 --> X8\n",
      "52. X48 --> X8\n",
      "53. X9 --> X11\n",
      "54. X9 --> X22\n",
      "55. X9 --> X33\n",
      "56. X9 --- X37\n",
      "57. X10 --> X17\n",
      "58. X19 --> X10\n",
      "59. X10 --- X26\n",
      "60. X32 --> X10\n",
      "61. X10 --> X34\n",
      "62. X40 --> X10\n",
      "63. X10 --> X45\n",
      "64. X11 --- X13\n",
      "65. X20 --> X11\n",
      "66. X22 --> X11\n",
      "67. X27 --> X11\n",
      "68. X30 --> X11\n",
      "69. X11 --> X31\n",
      "70. X42 --> X11\n",
      "71. X47 --> X11\n",
      "72. X49 --> X11\n",
      "73. X14 --> X12\n",
      "74. X26 --> X12\n",
      "75. X29 --> X12\n",
      "76. X30 --> X12\n",
      "77. X37 --> X12\n",
      "78. X13 --> X29\n",
      "79. X31 --> X13\n",
      "80. X13 --> X45\n",
      "81. X20 --> X14\n",
      "82. X36 --> X14\n",
      "83. X14 --> X40\n",
      "84. X14 --> X48\n",
      "85. X14 --> X51\n",
      "86. X15 --> X43\n",
      "87. X15 --> X49\n",
      "88. X51 --> X15\n",
      "89. X20 --> X16\n",
      "90. X16 --> X23\n",
      "91. X25 --> X16\n",
      "92. X27 --> X16\n",
      "93. X16 --- X28\n",
      "94. X16 --- X32\n",
      "95. X38 --> X16\n",
      "96. X45 --> X16\n",
      "97. X19 --> X17\n",
      "98. X33 --> X17\n",
      "99. X36 --> X17\n",
      "100. X49 --> X17\n",
      "101. X18 --- X24\n",
      "102. X18 --> X27\n",
      "103. X18 --> X28\n",
      "104. X18 --> X39\n",
      "105. X18 --> X43\n",
      "106. X47 --> X18\n",
      "107. X23 --> X19\n",
      "108. X24 --> X19\n",
      "109. X28 --> X19\n",
      "110. X19 --- X32\n",
      "111. X19 --> X34\n",
      "112. X19 --> X38\n",
      "113. X39 --> X19\n",
      "114. X42 --> X19\n",
      "115. X36 --> X20\n",
      "116. X20 --> X38\n",
      "117. X45 --> X20\n",
      "118. X21 --> X22\n",
      "119. X21 --> X25\n",
      "120. X30 --> X21\n",
      "121. X21 --> X40\n",
      "122. X41 --> X21\n",
      "123. X21 --> X48\n",
      "124. X22 --> X25\n",
      "125. X30 --> X22\n",
      "126. X37 --> X22\n",
      "127. X41 --> X22\n",
      "128. X26 --> X23\n",
      "129. X23 --- X27\n",
      "130. X38 --> X23\n",
      "131. X23 --> X39\n",
      "132. X46 --> X23\n",
      "133. X24 --> X32\n",
      "134. X24 --> X34\n",
      "135. X24 --> X38\n",
      "136. X24 --> X42\n",
      "137. X24 --> X43\n",
      "138. X47 --> X24\n",
      "139. X26 --> X25\n",
      "140. X41 --> X25\n",
      "141. X44 --> X25\n",
      "142. X48 --> X25\n",
      "143. X33 --> X26\n",
      "144. X39 --> X26\n",
      "145. X40 --> X26\n",
      "146. X45 --> X26\n",
      "147. X48 --> X26\n",
      "148. X29 --> X27\n",
      "149. X39 --> X27\n",
      "150. X46 --> X27\n",
      "151. X28 --- X32\n",
      "152. X38 --> X28\n",
      "153. X28 --> X39\n",
      "154. X44 --> X28\n",
      "155. X47 --> X28\n",
      "156. X41 --> X29\n",
      "157. X44 --> X29\n",
      "158. X45 --> X29\n",
      "159. X30 --> X33\n",
      "160. X36 --> X30\n",
      "161. X37 --> X30\n",
      "162. X35 --> X31\n",
      "163. X36 --> X31\n",
      "164. X32 --> X34\n",
      "165. X38 --> X32\n",
      "166. X43 --> X32\n",
      "167. X32 --> X48\n",
      "168. X37 --> X33\n",
      "169. X43 --> X34\n",
      "170. X46 --> X34\n",
      "171. X35 --- X50\n",
      "172. X36 --> X47\n",
      "173. X38 --> X39\n",
      "174. X43 --> X39\n",
      "175. X39 --> X44\n",
      "176. X40 --> X42\n",
      "177. X49 --> X41\n",
      "178. X47 --> X42\n",
      "179. X48 --> X42\n",
      "180. X44 --> X45\n",
      "181. X49 --> X44\n",
      "182. X46 --- X50\n",
      "183. X48 --> X47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "\n",
    "### Dataset\n",
    "df = pd.concat([LU_X, pd.Series(LU_y, name = 'Y', index = LU_X.index)], axis = 1)\n",
    "df = df.astype(float)\n",
    "data = df.values\n",
    "var_names = df.columns.tolist()\n",
    "\n",
    "### PC algorithm\n",
    "cg = pc(data, alpha = 0.05, labels = var_names, max_k = 2)\n",
    "print(cg.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10eb5cae-3911-4edc-9464-a3f497baaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(var_names).to_csv('LU_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18d670be-7932-4028-9787-76886f2d7c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causallearn.search.FCMBased import lingam\n",
    "\n",
    "model = lingam.DirectLiNGAM()\n",
    "model.fit(data)\n",
    "pd.DataFrame(model.adjacency_matrix_).to_csv('LU_lingam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c97a83d9-9fd8-420e-94f0-eaa78e1f8c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from notears.linear import notears_linear\n",
    "\n",
    "W_est = notears_linear(data, lambda1 = 0.0001, loss_type = 'l2')\n",
    "\n",
    "pd.DataFrame(W_est).to_csv('LU_notears.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
